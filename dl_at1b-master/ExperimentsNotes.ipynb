{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalisation \n",
    "\n",
    "Insipiration from [Keras-Resnet](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py) Implementation. \n",
    "\n",
    "```\n",
    "     x = layers.Conv2D(filters1, (1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "```\n",
    "\n",
    "Here we see a conv2d layer followed by batchnormalisation\n",
    "\n",
    "[Keras Normalisation](https://keras.io/layers/normalization/): the batch normalisation layer will normalise the activations of a pervioud layer at every batch. the normalisation will maintain the mean activation close to 0 and standard deviation close to 1. \n",
    "\n",
    "\n",
    "according to paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) this method addresses what is known as the internal covariate shift, when the distribution of layer's inputs change during training. the paper suggests that batch normalisation acives accuracy 14 times fewer traning in fewer steps. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data flow \n",
    "\n",
    "\n",
    "[Keras Data Generators](https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNEt\n",
    "\n",
    "good article on using Resnet: [Understanding and Coding a ResNet in Keras](https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code for [ResNet50 implementation on github](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py). \n",
    "\n",
    "In this sample, it is good to see how Resnet implements the conv_block, which serves as the building block for ResNet. conv_bloc contains 4 Conv2D layers, each layer will use he_normal initialisation and batch normalisation. activation is alwways relu. last conv layer is actually a short cut that takes the initial input of the building block. so essentiall, input goes through three layers, then at the last layer we add the input again (that is the shortcut layer. \n",
    "\n",
    "then identity_block has thre Conv2D with he_normal initalisation, batch normalisation and rely activation. there is no conv layer at shortcut \n",
    "\n",
    "The resnet network then does the following: \n",
    "- zero pad input image\n",
    "- conv2d (valid), batchnormalised, relu activation \n",
    "- zeropadding again \n",
    "- maxpooling \n",
    "\n",
    "Then starts repeating conv blocks with identitiy blocks in stages. \n",
    "\n",
    "\n",
    "Link to help page for [resnet50](https://keras.io/applications/#resnet).\n",
    "\n",
    "Api call: \n",
    "```python\n",
    "keras.applications.resnet.ResNet50(\n",
    "    include_top=True, \n",
    "    weights='imagenet', \n",
    "    input_tensor=None, \n",
    "    input_shape=None, \n",
    "    pooling=None, \n",
    "    classes=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict using resnet50 \n",
    "\n",
    "at first i tried to use resnet50 without any modification to predict from fashion mnist\n",
    "\n",
    "```python\n",
    "...\n",
    "train_images, train_labels = load_images_train_32_32_rgb()\n",
    "plt.imshow(train_images[6])\n",
    "plt.show()\n",
    "...\n",
    "new_model = ResNet50()\n",
    "new_image = skimage.transform.resize(\n",
    "    train_images[6], \n",
    "    (224,224), \n",
    "    mode='constant'\n",
    ")\n",
    "# some additional transforms to put the image in shape\n",
    "imgs_in = []\n",
    "imgs_in.append(new_image) # now this is an array in a list\n",
    "# make it a array\n",
    "imgs_in = np.array(imgs_in)\n",
    "print(imgs_in.shape)\n",
    "# this will produce (1,224,224,3)\n",
    "...\n",
    "pred_output = new_model.predict(resnet50.preprocess_input(imgs_in))\n",
    "# pred_output shape is (1,1000)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', resnet50.decode_predictions(pred_output, top=3)[0])\n",
    "## output is Predicted: [('n02504458', 'African_elephant', 0.5607367), ('n01871265', 'tusker', 0.3651714), ('n02504013', 'Indian_elephant', 0.073968664)]\n",
    "```\n",
    "\n",
    "the result is pretty bad, the test is to predict this one again after retraining resnet on the fashnion mnist and see how it performs. \n",
    "also note how the predicted output had 1000 classes, in our case we will retrain to predict 10 other classes from fashion mnist. \n",
    "\n",
    "the code and output are here [resnet predict example notebook](output_notebooks/resnet_predict_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning  (1)\n",
    "\n",
    "In the first attempt at transfer learning i removed the top layers, and added my own fully connected layers. \n",
    "\n",
    "\n",
    "```python \n",
    "\n",
    "# initial run was showing very high variance (acc on trainng is good, but acc on val is very poor)\n",
    "# so i added regulisation \n",
    "added_layers = GlobalAveragePooling2D()(added_layers)#  Flatten()(added_layers)\n",
    "added_layers = Dropout(0.7)(added_layers)\n",
    "added_layers = Dense(128, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(added_layers)\n",
    "added_layers = Activation('relu')(added_layers)\n",
    "# added_layers = BatchNormalization()(added_layers)\n",
    "\n",
    "preds = Dense(10, activation ='softmax')(added_layers)\n",
    "\n",
    "final_model = Model(input = base_model.input, outputs=preds)\n",
    "\n",
    "```\n",
    "all base_model layers where frozen -- mean that the new model will take the final output of the base, do all initial feature detection and the more complex ones (this is a problem) before applying my dense layers, the final result was very bad with val_acc == .1 \n",
    "\n",
    "full results [notebook is found here](output_notebooks/mnist_resnet_1.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (2) full trainng \n",
    "\n",
    "since the previous attempt was bad, i decided to redo the experiment but this time i am not freezing any layers. i did try to freeze all but last two, but that did not help. and i run this one on a smaller data set to see results quickly (just 1000 samples) \n",
    "\n",
    "\n",
    "this case was training 23 million params \n",
    "```\n",
    "Total params: 23,851,274\n",
    "Trainable params: 23,798,154\n",
    "Non-trainable params: 53,120\n",
    "```\n",
    "\n",
    "results are here [mnist_resnet_2_full_resnet_training](output_notebooks/mnist_resnet_2_full_resnet_training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (3) - freeze some and keep some \n",
    "\n",
    "I tried differnt settgings, freeze first 60, 20 and 10 layers \n",
    "\n",
    "The results are very depressing \n",
    "\n",
    "I even added another FC layer at the end, trained on the full data set to deal with variance issue, and trained for longer (150 epochs).\n",
    "\n",
    "\n",
    "training accuracy plateaued quickly, after 20 epochs, while val accuracy remained very low, around .1\n",
    "\n",
    "validation loss did not drop below 3.8 compared to training loss at .012\n",
    "\n",
    "so far, it seems that the resnet weights are heavily trained to recognise the ImageNet data set features, our data set seems to be very different for transfer learning to be able to adapt to it. \n",
    "\n",
    "\n",
    "explaination in [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) may suggest that transfering learning is appropriate in this case \n",
    "\n",
    "the output is in [mnist_resnet_3_freeze_some](output_notebooks/mnist_resnet_3_freeze_some.ipynb) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
